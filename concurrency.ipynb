{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22ea568c-065b-4304-8cf7-be6c20c001b0",
   "metadata": {},
   "source": [
    "The current implementation, where only IDs are stored in the UserActivities table and names are kept in separate reference tables (Component, Action,  Target), prevents **data inconsistency**. If a name changes (e.g., renaming a Component), it only needs to be updated in the reference table, and all related rows in UserActivities will automatically reflect the change through the foreign key relationship. This eliminates the need to update multiple rows in UserActivities, avoids potential mismatches, and ensures data consistency across the database. It also simplifies maintenance and reduces the risk of errors in large datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49ee0bdd-995a-4009-8266-7d96ca580b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Inserting Rows: 100%|███████████████████████████████████████████████████████| 145261/145261 [00:22<00:00, 6332.29row/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data inserted successfully using threads and batch writes.\n",
      "Database connections closed.\n",
      "Populated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import Column, Integer, String, DateTime, ForeignKey\n",
    "from sqlalchemy.orm import declarative_base, relationship\n",
    "\n",
    "# Define the base for SQLAlchemy\n",
    "Base = declarative_base()\n",
    "\n",
    "# Define normalized table schemas\n",
    "class User(Base):\n",
    "    __tablename__ = 'users'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    user_id = Column(Integer, unique=True, nullable=False)\n",
    "    activities = relationship('UserActivity', back_populates='user')\n",
    "    # user.activities  # Return all UserActivity records for a given User\n",
    "\n",
    "class Component(Base):\n",
    "    __tablename__ = 'components'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    name = Column(String, unique=True, nullable=False)\n",
    "    activities = relationship('UserActivity', back_populates='component')\n",
    "\n",
    "class Action(Base):\n",
    "    __tablename__ = 'actions'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    name = Column(String, unique=True, nullable=False)\n",
    "    activities = relationship('UserActivity', back_populates='action')\n",
    "\n",
    "class Target(Base):\n",
    "    __tablename__ = 'targets'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    name = Column(String, unique=True, nullable=False)\n",
    "    activities = relationship('UserActivity', back_populates='target')\n",
    "\n",
    "class UserActivity(Base):\n",
    "    __tablename__ = 'user_activities'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    user_id = Column(Integer, ForeignKey('users.id'), nullable=False)\n",
    "    datetime = Column(DateTime, nullable=False)\n",
    "    component_id = Column(Integer, ForeignKey('components.id'), nullable=False)\n",
    "    action_id = Column(Integer, ForeignKey('actions.id'), nullable=False)\n",
    "    target_id = Column(Integer, ForeignKey('targets.id'), nullable=True)\n",
    "\n",
    "    user = relationship('User', back_populates='activities')\n",
    "    # user_activity.user  # Returns the User record for a given UserActivity\n",
    "    component = relationship('Component', back_populates='activities')\n",
    "    action = relationship('Action', back_populates='activities')\n",
    "    target = relationship('Target', back_populates='activities')\n",
    "\n",
    "########################################################################################################################################################\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os, datetime\n",
    "import threading, queue\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.sql import text\n",
    "from sqlalchemy.orm import scoped_session, sessionmaker\n",
    "\n",
    "class DatabaseHandler:\n",
    "    def __init__(self, db_path='user_activities.db'):\n",
    "        \"\"\"Initialize the database with Write-Ahead Logging (WAL) mode.\"\"\"\n",
    "        try:\n",
    "\n",
    "            self.engine = create_engine(\n",
    "                f'sqlite:///{db_path}', # Create an SQLAlchemy Engine representing the database connection\n",
    "                connect_args={\n",
    "                    \"timeout\": 30  # Specifies the duration (in seconds) the connection will wait \n",
    "                                   # for a database lock to be released when another thread is using it.\n",
    "                },\n",
    "                pool_size=5,  # The number of persistent connections to maintain in a connection pool\n",
    "                max_overflow=10  # Allow up to 10 additional connections when all connections are in use.\n",
    "            )\n",
    "            \n",
    "            # create tables using SQLAlchemy metadata\n",
    "            Base.metadata.create_all(self.engine)\n",
    "    \n",
    "            # Enable WAL mode with raw connection\n",
    "            conn = self.engine.raw_connection()\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute('PRAGMA journal_mode=WAL')  # Enable WAL mode\n",
    "            cursor.close()\n",
    "            conn.close() \n",
    "    \n",
    "            # Scoped session for thread-safe database access\n",
    "            self.SessionFactory = sessionmaker(bind=self.engine)\n",
    "            self.scoped_session = scoped_session(self.SessionFactory)\n",
    "            \n",
    "            # A shared global cache\n",
    "            self.global_cache = {\n",
    "                \"user\": {},\n",
    "                \"component\": {},\n",
    "                \"action\": {},\n",
    "                \"target\": {}\n",
    "            }\n",
    "            self.lock = threading.Lock()  # A lock for thread-safe cache access\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error setting up the database: {e}\")\n",
    "            self.close()\n",
    "\n",
    "    def close(self):\n",
    "        \"\"\"Dispose of the database engine.\"\"\"\n",
    "        if self.scoped_session:\n",
    "            self.scoped_session.remove()  # Remove scoped sessions\n",
    "        self.engine.dispose()\n",
    "        print(\"Database connections closed.\")\n",
    "    \n",
    "    def _add_or_return_row(self, session, table, field, value, cache_key):\n",
    "        \"\"\"Retrieve or create an entry in the database with a shared cache and ON CONFLICT handling.\"\"\"\n",
    "        try:\n",
    "            with self.lock:\n",
    "                # Check the global cache\n",
    "                if value in self.global_cache[cache_key]:\n",
    "                    return self.global_cache[cache_key][value]\n",
    "    \n",
    "            # Use raw SQL with ON CONFLICT to handle duplicates\n",
    "            sql = text(f\"\"\"\n",
    "            INSERT INTO {table.__tablename__} ({field})\n",
    "            VALUES (:value)\n",
    "            ON CONFLICT ({field}) DO NOTHING\n",
    "            \"\"\")\n",
    "            session.execute(sql, {'value': value})\n",
    "            session.flush()  # Ensure the new row is available\n",
    "    \n",
    "            # Retrieve the ID (either newly inserted or existing)\n",
    "            entry = session.query(table).filter_by(**{field: value}).first()\n",
    "            if entry:\n",
    "                with self.lock:\n",
    "                    # Update global cache\n",
    "                    self.global_cache[cache_key][value] = entry.id\n",
    "                return entry.id\n",
    "    \n",
    "        except Exception as e:\n",
    "            session.rollback()\n",
    "            print(f\"Error in _add_or_return_row for model {table.__name__}, field {field}, value {value}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def worker_thread(self, data_chunk, write_queue, progress_bar):\n",
    "        \"\"\"Thread worker to process a chunk of data.\"\"\"\n",
    "        session = self.scoped_session()\n",
    "        activities = []\n",
    "    \n",
    "        try:\n",
    "            for _, row in data_chunk.iterrows():\n",
    "                # Attempt to add or retrieve IDs for dimensional tables\n",
    "                required_fields = [\n",
    "                    ('user_id', User, 'user_id', row['User_ID'], 'user'),\n",
    "                    ('component_id', Component, 'name', row['Component'], 'component'),\n",
    "                    ('action_id', Action, 'name', row['Action'], 'action'),\n",
    "                    ('target_id', Target, 'name', row['Target'], 'target'),\n",
    "                ]\n",
    "    \n",
    "                # UserActivity\n",
    "                record = {}\n",
    "                for field_name, table, column, value, cache_key in required_fields:\n",
    "                    if pd.notna(value):\n",
    "                        record[field_name] = self._add_or_return_row(session, table, column, value, cache_key)\n",
    "                        \n",
    "                        if record[field_name] is None:\n",
    "                            print(f\"Skipping row due to error in {field_name}: {row.to_dict()}\")\n",
    "                            break\n",
    "                            \n",
    "                else:\n",
    "                    # Uses a 'for-else' which runs only if the loop above completes without a break\n",
    "                    record['datetime'] = datetime=pd.to_datetime(row['DateTime'], format=\"%d/%m/%Y %H:%M\")\n",
    "                    activities.append(record)\n",
    "                    progress_bar.update(1)\n",
    "    \n",
    "            # Commit changes for dimensional tables\n",
    "            session.commit()\n",
    "    \n",
    "            # Pass the activities to the writer thread\n",
    "            write_queue.put(activities)\n",
    "    \n",
    "        except Exception as e:\n",
    "            session.rollback()\n",
    "            print(f\"Error in worker thread: {e}\")\n",
    "        finally:\n",
    "            session.close()\n",
    "\n",
    "\n",
    "    def writer_thread(self, write_queue):\n",
    "        \"\"\"Thread to handle batch writes to the database.\"\"\"\n",
    "        session = self.scoped_session()\n",
    "        while True:\n",
    "            batch = write_queue.get()\n",
    "            if batch is None:  # Stop signal\n",
    "                break\n",
    "\n",
    "            retry_count = 5  # Retry up to 5 times\n",
    "            while retry_count > 0:\n",
    "                try:\n",
    "                    session.bulk_save_objects([UserActivity(**activity) for activity in batch])\n",
    "                    session.commit()\n",
    "                    break\n",
    "                except Exception as e:\n",
    "                    session.rollback()\n",
    "                    retry_count -= 1\n",
    "                    if retry_count == 0:\n",
    "                        print(f\"Error in writer thread after retries: {e}\")\n",
    "                    else:\n",
    "                        print(f\"Retrying writer thread due to: {e}\")\n",
    "        session.close()\n",
    "        \n",
    "\n",
    "    def populate_concurrently(self, dataframe, num_threads=4):\n",
    "\n",
    "        # Keep track of worker threads\n",
    "        threads = [] \n",
    "        \n",
    "        # Determine the size of each chunk to balance workload and minimize thread contention.\n",
    "        chunk_size = max(len(dataframe) // (num_threads * 2), 1000) \n",
    "        \n",
    "        write_queue = queue.Queue() # Queue for passing data chunks to the writer thread\n",
    "\n",
    "        # A shared progress bar to visually track the number of rows inserted\n",
    "        with tqdm(total=len(dataframe), desc=\"Inserting Rows\", unit=\"row\") as progress_bar:\n",
    "            \n",
    "            #  A single writer thread manages manages batch writes to reduce contention\n",
    "            writer = threading.Thread(target=self.writer_thread, args=(write_queue,))\n",
    "            writer.start()\n",
    "\n",
    "            # Launch worker threads to process data chunks into database representation\n",
    "            for i in range(num_threads):\n",
    "                \n",
    "                start = i * chunk_size\n",
    "                end = (i + 1) * chunk_size if i < num_threads - 1 else len(dataframe)\n",
    "                # Map the data_chunk using the beginning and end indices\n",
    "                data_chunk = dataframe.iloc[start:end]\n",
    "\n",
    "                # Each thread processes a chunk of data and prepares it for insertion.\n",
    "                # The worker thread then passes the results to the write_queue.\n",
    "                thread = threading.Thread(target=self.worker_thread, args=(data_chunk, write_queue, progress_bar))\n",
    "                threads.append(thread)\n",
    "                thread.start()\n",
    "\n",
    "            # The join() method waits for threads to finish before continuing\n",
    "            for thread in threads:\n",
    "                thread.join()\n",
    "\n",
    "            # Notify writer_thread that all work is complete by sending None\n",
    "            write_queue.put(None)\n",
    "\n",
    "            writer.join()\n",
    "\n",
    "        print(\"Data inserted successfully using threads and batch writes.\")\n",
    "\n",
    "# Example usage\n",
    "db_path = 'user_activities.db'\n",
    "\n",
    "# Remove existing database\n",
    "\n",
    "if os.path.exists(db_path):\n",
    "    os.remove(db_path)\n",
    "\n",
    "dataframes = pd.read_csv('dataframes.csv')\n",
    "db = DatabaseHandler(db_path)\n",
    "db.populate_concurrently(dataframes, num_threads=4)\n",
    "db.close()\n",
    "\n",
    "print(\"Populated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c158b2-4e03-4e7b-bb40-7c239892148d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (py39)",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
